<!DOCTYPE html>
<html>
<head>
<title>Data Science for Social Good fellowship - University of Chicago</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link href="/css/bootstrap.min.css" rel="stylesheet">
<link href="/css/bootstrap-responsive.min.css" rel="stylesheet">
<link href="/css/custom.css" rel="stylesheet">
<link href="/css/site.css" rel="stylesheet">
<link href="/css/flat-ui.css" rel="stylesheet">
<script src="/js/modernizr.js"></script>
</head>
<body>
<!-- Navbar -->
<div class="navbar navbar-inverse navbar-fixed-top">
    <div class="navbar-inner">
        <div class="container">
            <!-- The Logo -->
            <button type="button" class="btn btn-navbar" data-toggle="collapse" data-target="#nav-collapse-03">
            </button>
            <a href="http://dssg.io" id="nav-logo" class="brand">
                <!-- <i class="fui-loop" style="vertical-align: bottom;"></i> -->
                Data Science for Social Good</a>
            <div class="nav-collapse collapse" id="nav-collapse-03">
                <ul class="nav">
                    <li>
                    <a href="http://dssg.io/people">
                        People
                    </a>
                    </li>
                    <li>
                    <a href="http://dssg.io/projects">
                        Projects
                    </a>
                    </li>
                    <li>
                    <a href="http://dssg.io/events">
                        Events
                    </a>
                    </li>
                    <li>
                    <a href="http://dssg.io/contact">
                        Get involved
                    </a>
                    </li>
                    <li>
                    <a href="http://dssg.io/blog">
                        Blog
                    </a>
                    </li>
                    <li>
                    <a href="http://dssg.io/media">
                        Media
                    </a>
                    </li>
                </ul> <!-- /nav -->
            </div>
        </div>
    </div>
</div>
<!-- Header -->

<div class="container">
    <div class="row">
        <div class="span1">
        </div>
        <div class="span10">
            <h2 class="page-title">
                Legislative Influence Detector (LID)
                <hr/>
            </h2>
                <p class="answer">
                Journalists, researchers, and concerned citizens would like to know who’s writing legislative bills, but trying to read those bills, let alone trace their source, is tedious and time consuming. This is especially true at the state and local levels, where arguably more important policy decisions are made every day.</p>

                <p>The Legislative Influence Detector (LID) helps watchdogs turn this mountain of data into digestable insights about the origin and diffusion of policy ideas, the effectiveness of various lobbying organizations, and the democratic nature of individual bills, all in near real time. LID uses more than 500,000 state bills (collected by the <a href="sunlightfoundation.com">Sunlight Foundation</a>) and 2,400 pieces of model legislation (collected by us, <a href="http://www.alecexposed.org/wiki/ALEC_Exposed">ALEC Exposed</a>, and other groups) for textual similarities and flags them for review. The user can then investigate the matches.</p>

                <p>The screenshot below shows an example. On the left-hand side is text from <a href="http://docs.legis.wisconsin.gov/2015/proposals/sb179">Wisconsin Senate Bill 179</a> (2015), which bans most abortions past the 19th week of pregnancy. On the right-hand side is SB 179's highest-ranked match, <a href="https://legiscan.com/LA/text/SB593/2012">Louisiana Senate Bill 593</a> (2012). The highlighting shows that these sections match almost perfectly. Where differences exist, they are usually numbers versus spelling (e.g. “16” versus “sixteen”) or section identifiers (e.g. “(b)” versus “(9)”).</p>

                <img src="/img/posts/sunlight-reuse.png" align="center">

                <BR>&nbsp;<BR> 

                <p>Other tools can help investigate the origins of legislative bills. Many journalists and researchers read bills manually. Others use Google. Humans read slowly, and Google returns lots of non-legislative results. Inspired by <a href="http://onlinelibrary.wiley.com/doi/10.1111/ajps.12175/abstract">Wilkerson, Smith, and Stramp (2015)</a> and <a href="http://projects.iq.harvard.edu/govposters/file/357876">Hertel-Fernandez and Kashin</a>, LID is faster and easier to use than existing tools.</p>




            <h4>How Does It Work?</h4>
                <p class="answer">When the user wants to find text matches for a legislative document, she copies and pastes that document text into LID's input box. LID then quickly scans all its documents for similar text and returns the best matches, highlighting the matching text.</p> 

                <p>NOTE: LID is not yet robust enough to handle public traffic. We ran all the documents we have through LID, stored the matches in files, and made them available for download below. We hope to make the interactive tool available to the public in the coming months.</p>

                <p>We use the <a href=https://en.wikipedia.org/wiki/Smith-Waterman_algorithm">Smith-Waterman local-alignment algorithm</a> to find matching text across documents. This algorithm grabs pieces of text from each document and compares each word, adding points for matches and subtracting points for mismatches.</p>

                <p>Unfortunately, the local-alignment algorithm is too slow to work on large sets of text, such as ours. Given our computational resources, it would likely takes thousands of years to finish analyzing these pieces of legislation. We sped the analysis by limiting the number of documents that need to be compared. Elasticsearch, our database of choice for this project, efficiently calculates <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html">Lucene scores</a>. When we use LID to search for a document, it quickly compares our document against all others and grabs the 100 most similar documents as measured by their Lucene scores. Then we run the local-alignment algorithm on those.
                    <!-- You can find the code <a href="github.com/dssg/policy_diffusion">here</a>. -->
                </p>

                <p>Many tools, such as the typical school plagiarism detector, use a "<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag of words</a>" approach, which is much faster. Bag of words compares two documents based on the similarity of the words they use but does not consider their order. For example, bag of words may consider "I kicked the ball," "The ball I kicked," and even "kicked the I ball" the same. We consider word order important to legislation: not only does it add information about the bill's contents; it also helps us find subsets of matching text, such as sections and sentences. Because bills rarely pass without significant changes, a local-alignment algorithm's granularity helps it find more matches.</p>

                


            <h4>How Do I Use the Datasets?</h4>
                <p class="answer">
                We provide data in two formats: <a href="https://en.wikipedia.org/wiki/Comma-separated_values">comma-separated values</a> (CSV) and <a href="https://en.wikipedia.org/wiki/JSON">JavaScript Object Notation</a> (JSON). Spreadsheet programs such as Excel can read CSVs, but you need more specialized programs to read JSON files, such as <a href="https://cran.r-project.org/web/packages/jsonlite/index.html">jsonlite</a> (for R), <a href="https://docs.python.org/3/library/json.html">json</a> (for Python), or <a href="https://stedolan.github.io/jq/">jq</a> (for the linux command line). We dropped some fields in the CSV (e.g. the text of the query document), but the JSON files have all the data.</p> 

                <p>These files contain the results for comparisons across states and between lobbying groups and states. We have not yet run comparisons within states. Many states reintroduce nearly entire bills, sometimes automatically, which means the matches are strong and the algorithm is slow. We plan to run the algorithm on all the bills soon.</p>

                <FONT FACE="'ff-tisa-web-pro', Georgia, Cambria, 'Times New Roman', Times, serif" SIZE=10px>Here are the fields:</FONT>
                    <ol>
                        <li><u>query_document_id</u>: The bill ID for the search document. The first two characters reference the state, the next set of characters reference the session (e.g. 2015-2016), and the last set of characters identify the bill.</li>
                        <li><u>comparison_document_id</u>: The bill ID for the comparison document.</li>
                        <li><u>lucene_score</u>: <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html">Lucene scores</a> are a measure of the match we found in step 1. The better the match, the higher the number. We only use these scores as a rough way to limit the number of comparison documents for step 2.</li>
                        <li><u>score</u>: A measure of the match we found in step 2. The numbers are a little hard to interpret directly, but longer the text match, the higher the score. If you order the results from highest to lowest, you'll have the best results at the top and the worst results at the bottom. 

                        Whatever Smith-Waterman cutoff you use, know that you're making a tradeoff: The higher the cutoff, the fewer false positives (incorrectly flagged matches) and the more false negatives (missed matches) you will get. We have found a decent set of matches only using scores over 100, but if you're worried about missing other bills, you can check the matches with scores under 100 too.</li>
                        <li><u>query_document_start</u>: The starting position of the matched text in the query document.</li>
                        <li><u>query_document_end</u>: The ending position of the matched text in the query document.</li>
                        <li><u>comparison_document_start</u>: The starting position of the matched text in the comparison document.</li>
                        <li><u>comparison_document_end</u>: The ending position of the matched text in the comparison document.</li>
                        <li><u>query_document_text</u>: The matched text in the query document.</li>
                        <li><u>comparison_document_text</u>: The matched text in the comparison document.</li>
                    </ol>
                

                <p>These results are not comprehensive. We will not find matches on bills that failed to make their way into Sunlight's database. We will not find model legislation that is not public. Lobbyists and legislators may avoid detection by rewriting bills. Although LID would not find these examples, it does make re-using text more difficult. LID helps establish a minimum number of matches.</p>



            <h4>Where Do I Download the Dataset?</h4>
                <p class="answer">We have two datasets: bill-to-bill comparisons and model legislation-to-bill comparisons. The former shows how bills spread across legislatures, and the latter show how ideas spread across lobbying groups and legislatures:
		          <ul> 
                    <li>Bill-to-bill comparisons in CSV format (updated through <a href="https://s3-us-west-2.amazonaws.com/policy-origination/bill_to_bill_alignments.csv">May, 2015</a>)</li>
			         <li>Bill-to-bill comparisons in JSON format (updated through <a href="https://s3-us-west-2.amazonaws.com/policy-origination/bill_to_bill_alignments.json">May, 2015</a>)</li>
                     <li>Model legislation-to-bill comparisons in CSV format (updated through <a href="https://s3-us-west-2.amazonaws.com/policy-origination/model_legislation_alignments.csv">May, 2015</a>)</li>
			         <li>Model legislation-to-bill comparisons in JSON format (updated through <a href="https://s3-us-west-2.amazonaws.com/policy-origination/model_legislation_alignments.json">May, 2015</a>)</li>
		          </ul>
                </p>

            <h4>Next Steps</h4>
                Much work remains to make LID as useful as possible. We are working on four improvements:
                <ol>
                    <li><ul>Look for all matches</ul>: To get results by the end of the DSSG, we only looked for matches across states, but many state legislatures re-introduce bills. We will re-run our documents through LID to find matches within states too.</li>
                    <li><ul>Look for new matches every day</ul>: State legislators introduce bills nearly every day. We would like to run those bills through LID every night and flag text similarities for review. We may set up an email system to alert interested users.</li>
                    <li><ul>Make the interactive tool public</ul>: At the moment, only we can run queries on LID. We would like to open the tool to the public. This requires increased system robustness.</li>
                    <li><ul>We would like to improve the matches</ul>: LID only counts exact matches. We would like to credit synonyms and other small differences.</li>
                </ol>


            <h4>Who are we?</h4>
            <p class="answer">
            <img src="/img/posts/sunlight15-team.png">
            </p>
        </div>
        <div class="span1">
        </div>
    </div>
</div>
<div class="container">
    <div class="row">
        <hr/>
        <h1 class="headline">
            <img id="logo" src="img/logo.png">
            <p>The Eric & Wendy Schmidt</p>
            Data Science for Social Good
            <p>Summer Fellowship</p>
            <img id="logo" src="/img/partners/uofc-new.jpg">
        </h1>
        <hr/>
    </div>
</div>

</body>
</html>
